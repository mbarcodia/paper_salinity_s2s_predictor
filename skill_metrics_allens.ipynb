{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Skill Metrics for certain lead week\n",
    "## Test data is unbalanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 09:34:48.162890: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from network_arch.ipynb\n",
      "importing Jupyter notebook from metrics.ipynb\n",
      "importing Jupyter notebook from plot.ipynb\n",
      "importing Jupyter notebook from settings.ipynb\n",
      "importing Jupyter notebook from functions_misc.ipynb\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: marcodia\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import math\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "import import_ipynb\n",
    "import sys\n",
    "import os \n",
    "\n",
    "import network_arch as network\n",
    "import metrics\n",
    "import plot as plot\n",
    "import settings\n",
    "import functions_misc as fnc\n",
    "\n",
    "\n",
    "from cartopy import config\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from shapely.geometry.polygon import LinearRing\n",
    "\n",
    "import pop_tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAKE THE NN ARCHITECTURE\n",
    "def make_model():\n",
    "    # Define and train the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = network.defineNN(HIDDENS,\n",
    "                             input1_shape = X_train.shape[1],\n",
    "                             output_shape=NLABEL,\n",
    "                             ridge_penalty1=RIDGE1,\n",
    "                             dropout=DROPOUT,\n",
    "                             act_fun='relu',\n",
    "                             network_seed=NETWORK_SEED)\n",
    "    \n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy()    \n",
    "    model.compile(\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate=LR_INIT),\n",
    "                  loss = loss_function,\n",
    "                  metrics = [\n",
    "                      tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None),\n",
    "                      metrics.PredictionAccuracy(NLABEL)\n",
    "                      ]\n",
    "                  )           \n",
    "    return model, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mam(month):\n",
    "    return np.logical_and(month>=3, month<=5)\n",
    "\n",
    "def is_may(month):\n",
    "    return np.logical_and(month>=5, month<=5)\n",
    "\n",
    "def is_aug(month):\n",
    "    return np.logical_and(month>=8, month<=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# XAI functions\n",
    "#.............................................\n",
    "\n",
    "# before calling these functions in your notebook, make sure you have defined the tensorflow object \"model\". \n",
    "# The \"model\" is the machine learning model (e.g., neural network) that you want to explain. \n",
    "\n",
    "# Code author of XAI functions: Libby Barnes and Tony Mamalakis\n",
    "\n",
    "def get_gradients(inputs, top_pred_idx=None):\n",
    "    \"\"\"Computes the gradients of outputs w.r.t input image.\n",
    "\n",
    "    Args:\n",
    "        inputs: 2D/3D/4D matrix of samples\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.\n",
    "\n",
    "    Returns:\n",
    "        Gradients of the predictions w.r.t img_input\n",
    "    \"\"\"\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)\n",
    "        \n",
    "        # Run the forward pass of the layer and record operations\n",
    "        # on GradientTape.\n",
    "        preds = model(inputs, training=False)  \n",
    "        \n",
    "        # For classification, grab the top class\n",
    "        if top_pred_idx is not None:\n",
    "            preds = preds[:, top_pred_idx]\n",
    "        \n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.        \n",
    "    grads = tape.gradient(preds, inputs)\n",
    "    return grads\n",
    "\n",
    "def get_integrated_gradients(inputs, baseline=None, num_steps=50, top_pred_idx=None):\n",
    "    \"\"\"Computes Integrated Gradients for a prediction.\n",
    "\n",
    "    Args:\n",
    "        inputs (ndarray): 2D/3D/4D matrix of samples\n",
    "        baseline (ndarray): The baseline image to start with for interpolation\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.            \n",
    "\n",
    "    Returns:\n",
    "        Integrated gradients w.r.t input image\n",
    "    \"\"\"\n",
    "    # If baseline is not provided, start with zeros\n",
    "    # having same size as the input image.\n",
    "    if baseline is None:\n",
    "        input_size = np.shape(inputs)[1:]\n",
    "        baseline = np.zeros(input_size).astype(np.float32)\n",
    "    else:\n",
    "        baseline = baseline.astype(np.float32)\n",
    "\n",
    "    # 1. Do interpolation.\n",
    "    inputs = inputs.astype(np.float32)\n",
    "    interpolated_inputs = [\n",
    "        baseline + (step / num_steps) * (inputs - baseline)\n",
    "        for step in range(num_steps + 1)\n",
    "    ]\n",
    "    interpolated_inputs = np.array(interpolated_inputs).astype(np.float32)\n",
    "\n",
    "    # 3. Get the gradients\n",
    "    grads = []\n",
    "    for i, x_data in enumerate(interpolated_inputs):\n",
    "        grad = get_gradients(x_data, top_pred_idx=top_pred_idx)\n",
    "        grads.append(grad)\n",
    "    grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n",
    "\n",
    "    # 4. Approximate the integral using the trapezoidal rule\n",
    "    grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "    avg_grads = tf.reduce_mean(grads, axis=0)\n",
    "\n",
    "    # 5. Calculate integrated gradients and return\n",
    "    integrated_grads = (inputs - baseline) * avg_grads\n",
    "    return integrated_grads\n",
    "\n",
    "def random_baseline_integrated_gradients(inputs, num_steps=50, num_runs=5, top_pred_idx=None):\n",
    "    \"\"\"Generates a number of random baseline images.\n",
    "\n",
    "    Args:\n",
    "        inputs (ndarray): 2D/3D/4D matrix of samples\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        num_runs: number of baseline images to generate\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.      \n",
    "\n",
    "    Returns:\n",
    "        Averaged integrated gradients for `num_runs` baseline images\n",
    "    \"\"\"\n",
    "    # 1. List to keep track of Integrated Gradients (IG) for all the images\n",
    "    integrated_grads = []\n",
    "\n",
    "    # 2. Get the integrated gradients for all the baselines\n",
    "    for run in range(num_runs):\n",
    "        baseline = np.zeros(np.shape(inputs)[1:])\n",
    "        for i in np.arange(0,np.shape(baseline)[0]):\n",
    "            j = np.random.choice(np.arange(0,np.shape(inputs)[0]))\n",
    "            baseline[i] = inputs[j,i]\n",
    "\n",
    "        igrads = get_integrated_gradients(\n",
    "            inputs=inputs,\n",
    "            baseline=baseline,\n",
    "            num_steps=num_steps,\n",
    "        )\n",
    "        integrated_grads.append(igrads)\n",
    "\n",
    "    # 3. Return the average integrated gradients for the image\n",
    "    integrated_grads = tf.convert_to_tensor(integrated_grads)\n",
    "    return tf.reduce_mean(integrated_grads, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lon = 265 \n",
    "min_lat = 8 \n",
    "max_lon = 320 \n",
    "max_lat = 50 \n",
    "\n",
    "rolling_input_window_X = 3\n",
    "rolling_input_window_Y = 1\n",
    "\n",
    "lambda_value = [0,7,14,21,28,35,42,49,56,63,70,77,84]\n",
    "fill_val = np.arange(0,120)\n",
    "\n",
    "poisson_weights = np.zeros((len(lambda_value),len(fill_val)))\n",
    "k_lead = 119 \n",
    "\n",
    "num_ens_test = 5\n",
    "exp_ind = 0\n",
    "threat_score_l8 = np.zeros(num_ens_test)\n",
    "gilbert_skill_score_l8 = np.zeros(num_ens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcodia/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 265)]             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 265)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 160)               42560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 192)               30912     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 386       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,858\n",
      "Trainable params: 73,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "385/385 [==============================] - 0s 1ms/step\n",
      "Threat Score: 0.13618290258449303\n",
      "Gilbert Skill Score: 0.02796420581655481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcodia/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 265)]             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 265)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 160)               42560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 192)               30912     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 386       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,858\n",
      "Trainable params: 73,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "385/385 [==============================] - 0s 1ms/step\n",
      "Threat Score: 0.2806679511881824\n",
      "Gilbert Skill Score: 0.0651085141903172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcodia/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 265)]             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 265)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 160)               42560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 192)               30912     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 386       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,858\n",
      "Trainable params: 73,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "385/385 [==============================] - 0s 994us/step\n",
      "Threat Score: 0.18595927116827438\n",
      "Gilbert Skill Score: -0.004629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcodia/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 265)]             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 265)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 160)               42560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 192)               30912     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 386       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,858\n",
      "Trainable params: 73,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "385/385 [==============================] - 0s 1ms/step\n",
      "Threat Score: 0.20256410256410257\n",
      "Gilbert Skill Score: 0.005754475703324808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcodia/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 265)]             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 265)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 160)               42560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 192)               30912     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 386       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,858\n",
      "Trainable params: 73,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "385/385 [==============================] - 0s 986us/step\n",
      "Threat Score: 0.25398773006134967\n",
      "Gilbert Skill Score: 0.043273013375295044\n"
     ]
    }
   ],
   "source": [
    "for exp in ['exp_8/exp_800', 'exp_8/exp_801','exp_8/exp_802','exp_8/exp_803','exp_8/exp_804']:\n",
    "    EXPERIMENT = exp     \n",
    "\n",
    "    ddir_X = '/Users/marcodia/Research/Data/global_daily_anomalies/'\n",
    "    ddir_Y = '/Users/marcodia/Research/Data/processed_fields/precip_data/'\n",
    "    ddir_out = '/Users/marcodia/Research/salinity_s2s/experiments/no_arctic/exp_8/' \n",
    "    ddir_MODEL = ddir_out\n",
    "    ddir_saveinfo = '/Users/marcodia/Research/salinity_s2s/experiments/no_arctic/evaluations/'\n",
    "\n",
    "    params = settings.get_settings(EXPERIMENT)\n",
    "\n",
    "    PREDICTOR_VAR  = params['PREDICTOR_VAR']           \n",
    "    PREDICTAND_VAR = params['PREDICTAND_VAR']              \n",
    "    REGION_TOR     = params['REGION_TOR']          \n",
    "    REGION_TAND    = params['REGION_TAND']            \n",
    "    training_ens   = params['training_ens']            \n",
    "    validation_ens = params['validation_ens']           \n",
    "    testing_ens    = params['testing_ens']           \n",
    "    train_list     = params['train_list']\n",
    "    val_list       = params['val_list']\n",
    "    lead           = params['lead']            \n",
    "    days_average   = params['days_average']            \n",
    "    GLOBAL_SEED    = params['GLOBAL_SEED']            \n",
    "    HIDDENS        = params['HIDDENS']          \n",
    "    DROPOUT        = params['DROPOUT']            \n",
    "    RIDGE1         = params['RIDGE1']                    \n",
    "    LR_INIT        = params['LR_INIT']\n",
    "    BATCH_SIZE     = params['BATCH_SIZE']           \n",
    "    RANDOM_SEED    = params['RANDOM_SEED']            \n",
    "    act_fun        = params['act_fun']            \n",
    "    N_EPOCHS       = params['N_EPOCHS']           \n",
    "    PATIENCE       = params['PATIENCE']   \n",
    "    CLASS_WEIGHT   = 'none'\n",
    "\n",
    "    window_size = days_average\n",
    "\n",
    "    lead_weeks = int(lead/7)\n",
    "\n",
    "    #>>>>>SET UP <<<<<<<<<<<<<<<\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    random.seed(GLOBAL_SEED)\n",
    "    tf.compat.v1.random.set_random_seed(GLOBAL_SEED)\n",
    "\n",
    "    NLABEL = 2\n",
    "\n",
    "    YEARS = '1850-1949'\n",
    "    STRT_I = pd.to_datetime('01-01-1850')\n",
    "    END_I   = pd.to_datetime('12-31-1949')  + dt.timedelta(days=1)\n",
    "\n",
    "    time_range = xr.cftime_range(str(STRT_I)[:10], str(END_I)[:10],calendar = 'noleap') #[0:10] corresponds to full datestamp\n",
    "    #time_range_szn = time_range.where(fnc.is_mjja(time_range.month)).dropna()\n",
    "    TIME_ALL = xr.DataArray(time_range + dt.timedelta(days=0), dims=['time'])     \n",
    "\n",
    "    STRT_2 = pd.to_datetime('05-01-1850')\n",
    "    END_2   = pd.to_datetime('08-31-1949')  + dt.timedelta(days=1)\n",
    "    time_range2 = xr.cftime_range(str(STRT_2)[:10], str(END_2)[:10],calendar = 'noleap') #[0:10] corresponds to full datestamp\n",
    "    time_range_2 = time_range2.where(fnc.is_mjja(time_range2.month)).dropna()\n",
    "    TIME_X = xr.DataArray(time_range_2 + dt.timedelta(days=0), dims=['time'])   \n",
    "\n",
    "    TIME_Y = xr.DataArray(time_range_2 + dt.timedelta(days=lead+days_average), dims=['time'])  #below comment explains time segmentation\n",
    "\n",
    "\n",
    "    # ----- X TRAINING ------\n",
    "    count = 0 \n",
    "    for i in train_list:\n",
    "        X_finame = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+i+'_dailyanom_detrend.nc'\n",
    "        X_all_full = xr.open_dataarray(ddir_X+X_finame)\n",
    "        X1 = X_all_full.where(X_all_full.time == TIME_ALL, drop=True)\n",
    "        X = X1.sel(lat=slice(min_lat,max_lat), lon=slice(min_lon,max_lon))\n",
    "\n",
    "        X_nptime = np.array(X.time)                 #for some annoying reason, it needed to be converted to numpy for creating DataArray   \n",
    "        X_nplat = np.array(X.lat)\n",
    "        X_nplon = np.array(X.lon)\n",
    "        del X_all_full \n",
    "\n",
    "        if count == 0: # don't rewrite empty matrix each time \n",
    "            X_all1_FULL = xr.DataArray(np.zeros((len(train_list),X.shape[0],X.shape[1],X.shape[2]))+np.nan,\n",
    "                                 dims = ['ens','time','lat','lon'],\n",
    "                                 coords = [('ens',np.arange(0,len(train_list))),('time', X_nptime),('lat',X_nplat),('lon',X_nplon)])\n",
    "\n",
    "        X_all1_FULL[count,:,:,:] = X   \n",
    "\n",
    "        count = count+1\n",
    "        del X\n",
    "\n",
    "    X_all1_ROLL = X_all1_FULL.rolling(time=(rolling_input_window_X),center=False).mean()\n",
    "    X_all1_inputtime1 = X_all1_ROLL.where(X_all1_ROLL.time == TIME_X, drop=True)\n",
    "    X_all1_inputtime = X_all1_inputtime1.dropna(dim='time', how = 'all')\n",
    "\n",
    "    Xtrain1 = X_all1_inputtime.stack(time_all=('ens','time')) # lat,lon,time*8 (8= number of training ens members) \n",
    "    Xtrain1 = Xtrain1.transpose('time_all','lat','lon') # time*8,lat,lon\n",
    "    Xtrain_std1 = np.std(Xtrain1,axis=0)\n",
    "    Xtrain_mean1 = np.mean(Xtrain1,axis=0)\n",
    "    Xtrain_full = (Xtrain1-Xtrain_mean1)/Xtrain_std1\n",
    "\n",
    "    del X_all1_ROLL, X_all1_inputtime1, X1, Xtrain1\n",
    "\n",
    "    ## Apply Mask\n",
    "\n",
    "    # Define latitude and longitude ranges that correspond to the Pacific Ocean\n",
    "    pacific_lat_range = slice(0, 3)  # Adjust the latitudes as needed\n",
    "    pacific_lon_range = slice(0, 5)  # Adjust the longitudes as needed\n",
    "\n",
    "    # Create a mask for the Pacific Ocean\n",
    "    mask = np.zeros(Xtrain_full.shape[1:], dtype=bool)\n",
    "    mask[pacific_lat_range, pacific_lon_range] = True\n",
    "\n",
    "    # Expand the mask to match the 'time' dimension\n",
    "    mask_expanded = np.repeat(mask[None, :, :], len(Xtrain_full.time_all), axis=0)\n",
    "\n",
    "    # Apply the mask to the dataset\n",
    "    Xtrain = Xtrain_full.where(~mask_expanded, np.nan)\n",
    "\n",
    "    # ---------- X VALIDATION----------\n",
    "    count = 0 \n",
    "    for i in val_list:\n",
    "        X_finame = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+i+'_dailyanom_detrend.nc'\n",
    "        X_all_full = xr.open_dataarray(ddir_X+X_finame)\n",
    "        X1 = X_all_full.where(X_all_full.time == TIME_ALL, drop=True)\n",
    "        X = X1.sel(lat=slice(min_lat,max_lat), lon=slice(min_lon,max_lon))\n",
    "\n",
    "        X_nptime = np.array(X.time)                 #for some annoying reason, it needed to be converted to numpy for creating DataArray   \n",
    "        X_nplat = np.array(X.lat)\n",
    "        X_nplon = np.array(X.lon)\n",
    "        del X_all_full \n",
    "\n",
    "        if count == 0: # don't rewrite empty matrix each time \n",
    "            X_val1_FULL = xr.DataArray(np.zeros((len(val_list),X.shape[0],X.shape[1],X.shape[2]))+np.nan,\n",
    "                                 dims = ['ens','time','lat','lon'],\n",
    "                                 coords = [('ens',np.arange(0,len(val_list))),('time', X_nptime),('lat',X_nplat),('lon',X_nplon)])\n",
    "\n",
    "        X_val1_FULL[count,:,:,:] = X   \n",
    "\n",
    "        count = count+1\n",
    "        del X\n",
    "\n",
    "    X_val1_ROLL = X_val1_FULL.rolling(time=(rolling_input_window_X),center=False).mean()\n",
    "    X_val1_inputtime1 = X_val1_ROLL.where(X_val1_ROLL.time == TIME_X, drop=True)\n",
    "    X_val1_inputtime = X_val1_inputtime1.dropna(dim='time', how = 'all')\n",
    "    Xval1 = X_val1_inputtime.stack(time_all=('ens','time')) # lat,lon,time*8 (8= number of training ens members) \n",
    "    Xval1 = Xval1.transpose('time_all','lat','lon') # time*8,lat,lon\n",
    "\n",
    "    Xval_full = (Xval1 - Xtrain_mean1)/Xtrain_std1\n",
    "\n",
    "    # Create a mask for the Pacific Ocean\n",
    "    mask = np.zeros(Xval_full.shape[1:], dtype=bool)\n",
    "    mask[pacific_lat_range, pacific_lon_range] = True\n",
    "\n",
    "    # Expand the mask to match the 'time' dimension\n",
    "    mask_expanded = np.repeat(mask[None, :, :], len(Xval_full.time_all), axis=0)\n",
    "\n",
    "    # Apply the mask to the dataset\n",
    "    Xval = Xval_full.where(~mask_expanded, np.nan)\n",
    "\n",
    "    # ---------- X TESTING----------\n",
    "    X_finame1  = PREDICTOR_VAR+'_'+REGION_TOR+'_'+YEARS+'_'+'ens'+str(testing_ens)+'_dailyanom_detrend.nc'\n",
    "    Xtest1_full = xr.open_dataarray(ddir_X+X_finame1)\n",
    "    Xtest_1= Xtest1_full.where(Xtest1_full.time == TIME_ALL, drop=True)\n",
    "    Xtest1 = Xtest_1.sel(lat=slice(min_lat,max_lat), lon=slice(min_lon,max_lon))\n",
    "\n",
    "    X_test1_ROLL = Xtest1.rolling(time=(rolling_input_window_X),center=False).mean()\n",
    "    X_test1_inputtime1 = X_test1_ROLL.where(X_test1_ROLL.time == TIME_X, drop=True)\n",
    "    X_test1_inputtime = X_test1_inputtime1.dropna(dim='time', how = 'all')\n",
    "\n",
    "    Xtest_full = (X_test1_inputtime - Xtrain_mean1)/Xtrain_std1\n",
    "\n",
    "    # Create a mask for the Pacific Ocean\n",
    "    mask = np.zeros(Xtest_full.shape[1:], dtype=bool)\n",
    "    mask[pacific_lat_range, pacific_lon_range] = True\n",
    "\n",
    "    # Expand the mask to match the 'time' dimension\n",
    "    mask_expanded = np.repeat(mask[None, :, :], len(Xtest_full.time), axis=0)\n",
    "\n",
    "    # Apply the mask to the dataset\n",
    "    Xtest = Xtest_full.where(~mask_expanded, np.nan)\n",
    "\n",
    "    del Xtrain_std1, X_test1_inputtime, X_test1_inputtime1, X_test1_ROLL, Xtest1, Xtest_1, Xtest1_full, Xtrain_mean1, X_val1_inputtime, X_val1_ROLL\n",
    "\n",
    "    ## Apply Poisson Weighting\n",
    "\n",
    "\n",
    "    # Calculate Poisson weights for lambda=14 and max value=30\n",
    "    count = 0\n",
    "    for l in lambda_value:\n",
    "        poisson_weights[count,:] = fnc.calculate_poisson_weights(l, k_lead)\n",
    "        count += 1\n",
    "\n",
    "    poisson_weights_T = np.transpose(poisson_weights)\n",
    "\n",
    "    poisson_use = poisson_weights_T[:,lead_weeks]   #THIS NEEDS TO BE SPECIFIED BASED ON LEAD TIME\n",
    "\n",
    "    # Example usage:\n",
    "    weight_vector = poisson_use  # Symmetric weights for a centered rolling average\n",
    "\n",
    "    center_element = lead\n",
    "\n",
    "    #%% ----- Y TRAINING--------\n",
    "\n",
    "    count = 0\n",
    "    for i in train_list:\n",
    "        Ytrain_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_'+YEARS+'_ens'+str(i)+'_'+str(window_size)+'daysum.nc'\n",
    "\n",
    "        Y_all_full = xr.open_dataarray(ddir_Y+Ytrain_finame)\n",
    "        Y = Y_all_full.where(Y_all_full.time == TIME_ALL, drop=True)\n",
    "\n",
    "        Y_nptime = np.array(Y.time)                 \n",
    "        del Y_all_full \n",
    "\n",
    "        if count == 0: # don't rewrite empty matrix each time \n",
    "            Y_all = xr.DataArray(np.zeros((len(train_list),Y.shape[0]))+np.nan,\n",
    "                                 dims = ['ens','time'],\n",
    "                                 coords = [('ens',np.arange(0,len(train_list))),('time', Y_nptime)])\n",
    "\n",
    "            poiss_weighted_rolling_avg = xr.DataArray(np.zeros((len(train_list),Y.shape[0]))+np.nan,\n",
    "                                  dims = ['ens','time'],\n",
    "                                coords = [('ens',np.arange(0,len(train_list))),('time', Y_nptime)])\n",
    "\n",
    "        Y_all[count,:] = Y   \n",
    "\n",
    "        centered_weighted_rolling_avg = fnc.uncentered_weighted_rolling_average(Y_all[count,:], weight_vector,center_element)\n",
    "        poiss_weighted_rolling_avg[count,:] = centered_weighted_rolling_avg   \n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "    Y_all1_inputtime1 = poiss_weighted_rolling_avg.where(poiss_weighted_rolling_avg.time == TIME_Y, drop=True)\n",
    "    Y_all1_inputtime = Y_all1_inputtime1.dropna(dim='time', how = 'all')\n",
    "    np.arange(Y_all1_inputtime.shape[0])\n",
    "\n",
    "    YP_nptime = np.array(Y_all1_inputtime.time)\n",
    "    Y_all_perc = xr.DataArray(np.zeros((len(train_list),Y_all1_inputtime.shape[1]))+np.nan,\n",
    "                                 dims = ['ens','time'],\n",
    "                                 coords = [('ens',np.arange(0,len(train_list))),('time', YP_nptime)])\n",
    "\n",
    "    for i in np.arange(Y_all1_inputtime.shape[0]):\n",
    "        heavy_val = np.percentile(Y_all1_inputtime[i,:], 80)\n",
    "        Y_all_perc[i,:] = (Y_all1_inputtime[i,:] >= heavy_val).astype(int) #+ (Y_use >= mod_val).astype(int)\n",
    "\n",
    "    Ytrain = Y_all_perc.stack(time_all=('ens','time'))\n",
    "    # How often does our data fall into each category? This is just for the last ensemble member in training\n",
    "    calcpercent = lambda cat: str((np.sum(np.array(Ytrain) == cat)/len(Ytrain)*100).astype(int))\n",
    "\n",
    "    # Print out the sizes of each class\n",
    "    # print('Frequency for each Precip Category')\n",
    "    # print('Light: ' + calcpercent(0) + '%')\n",
    "    # print('Heavy: ' + calcpercent(1) + '%')\n",
    "\n",
    "    #%% ----- Y VALIDATION--------\n",
    "\n",
    "    count = 0\n",
    "    for i in val_list:\n",
    "        Yval_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_'+YEARS+'_ens'+str(i)+'_'+str(window_size)+'daysum.nc'\n",
    "\n",
    "        Y_all_full = xr.open_dataarray(ddir_Y+Yval_finame)\n",
    "        Y = Y_all_full.where(Y_all_full.time == TIME_ALL, drop=True)\n",
    "\n",
    "        Y_nptime = np.array(Y.time)                 \n",
    "        del Y_all_full \n",
    "\n",
    "     #   $$$$$$$\n",
    "        if count == 0: # don't rewrite empty matrix each time \n",
    "            Y_all = xr.DataArray(np.zeros((len(val_list),Y.shape[0]))+np.nan,\n",
    "                                 dims = ['ens','time'],\n",
    "                                 coords = [('ens',np.arange(0,len(val_list))),('time', Y_nptime)])\n",
    "\n",
    "            val_poiss_weighted_rolling_avg = xr.DataArray(np.zeros((len(val_list),Y.shape[0]))+np.nan,\n",
    "                                  dims = ['ens','time'],\n",
    "                                coords = [('ens',np.arange(0,len(val_list))),('time', Y_nptime)])\n",
    "\n",
    "        Y_all[count,:] = Y   \n",
    "\n",
    "        val_centered_weighted_rolling_avg = fnc.uncentered_weighted_rolling_average(Y_all[count,:], weight_vector, center_element)\n",
    "        val_poiss_weighted_rolling_avg[count,:] = val_centered_weighted_rolling_avg   \n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "    Y_val1_inputtime1 = val_poiss_weighted_rolling_avg.where(val_poiss_weighted_rolling_avg.time == TIME_Y, drop=True)\n",
    "    Y_val1_inputtime = Y_val1_inputtime1.dropna(dim='time', how = 'all')\n",
    "\n",
    "    YP_nptime = np.array(Y_all1_inputtime.time)\n",
    "    Y_val_perc = xr.DataArray(np.zeros((len(val_list),Y_all1_inputtime.shape[1]))+np.nan,\n",
    "                                 dims = ['ens','time'],\n",
    "                                 coords = [('ens',np.arange(0,len(val_list))),('time', YP_nptime)])\n",
    "\n",
    "    for i in np.arange(Y_val1_inputtime.shape[0]):\n",
    "        heavy_val = np.percentile(Y_val1_inputtime[i,:], 80)\n",
    "        Y_val_perc[i,:] = (Y_val1_inputtime[i,:] >= heavy_val).astype(int) #+ (Y_use >= mod_val).astype(int)\n",
    "\n",
    "    Yval = Y_val_perc.stack(time_all=('ens','time'))\n",
    "    # How often does our data fall into each category? This is just for the last ensemble member in validation\n",
    "    calcpercent = lambda cat: str((np.sum(np.array(Yval) == cat)/len(Yval)*100).astype(int))\n",
    "\n",
    "    # Print out the sizes of each class\n",
    "    # print('Frequency for each Precip Category')\n",
    "    # print('Light: ' + calcpercent(0) + '%')\n",
    "    # print('Heavy: ' + calcpercent(1) + '%')\n",
    "\n",
    "\n",
    "    # ----- Y TESTING --------\n",
    "    Ytest_finame = PREDICTAND_VAR+'_'+REGION_TAND+'_'+YEARS+'_ens'+str(testing_ens)+'_'+str(window_size)+'daysum.nc'\n",
    "\n",
    "    Y_test_full = xr.open_dataarray(ddir_Y+Ytest_finame)\n",
    "    Y = Y_test_full.where(Y_test_full.time == TIME_ALL, drop=True)\n",
    "    Y_nptime = np.array(Y.time)\n",
    "\n",
    "    c_temp = xr.DataArray(np.zeros(Y.shape[0])+np.nan,\n",
    "                                  dims = ['time'],\n",
    "                                coords = [('time', Y_nptime)])\n",
    "\n",
    "    c_temp[:] = fnc.uncentered_weighted_rolling_average(Y,weight_vector,center_element)\n",
    "\n",
    "    Y_test1_inputtime1 = c_temp.where(c_temp.time == TIME_Y, drop=True)\n",
    "    Y_test1_inputtime = Y_test1_inputtime1.dropna(dim='time', how = 'all')\n",
    "\n",
    "    heavy_val = np.percentile(Y_test1_inputtime, 80)\n",
    "    Ytest = (Y_test1_inputtime >= heavy_val).astype(int) \n",
    "\n",
    "    calcpercent = lambda cat: str((np.sum(np.array(Ytest) == cat)/len(Ytest)*100).astype(int))\n",
    "\n",
    "    # Print out the sizes of each class\n",
    "    # print('Frequency for each Precip Category')\n",
    "    # print('Light: ' + calcpercent(0) + '%')\n",
    "    # print('Heavy: ' + calcpercent(1) + '%')\n",
    "\n",
    "    #Balance Classes\n",
    "    #Training\n",
    "    # make Yval have equal 0s and 1s so that random chance is 50%\n",
    "    n_valzero = np.shape(np.where(Ytrain==0)[0])[0]\n",
    "    n_valone  = np.shape(np.where(Ytrain==1)[0])[0]\n",
    "    i_valzero = np.where(Ytrain==0)[0]\n",
    "    i_valone  = np.where(Ytrain==1)[0]\n",
    "\n",
    "    if n_valone > n_valzero:\n",
    "        isubset_valone = np.random.choice(i_valone,size=n_valzero,replace=False)\n",
    "        i_valnew = np.sort(np.append(i_valzero,isubset_valone))\n",
    "        Y_train_1D = Ytrain.isel(time_all=i_valnew,drop=True)\n",
    "        X_train_w_NANS  = Xtrain[i_valnew]#.stack(z=('lat','lon'))\n",
    "    elif n_valone < n_valzero:\n",
    "        isubset_valzero = np.random.choice(i_valzero,size=n_valone,replace=False)\n",
    "        i_valnew = np.sort(np.append(isubset_valzero,i_valone))\n",
    "        Y_train_1D = Ytrain.isel(time_all=i_valnew,drop=True)\n",
    "        X_train_w_NANS  = Xtrain[i_valnew]#.stack(z=('lat','lon'))\n",
    "    else:\n",
    "        X_train_w_NANS = Xtrain#.stack(z=('lat','lon'))\n",
    "\n",
    "    #Validation\n",
    "    # make Yval have equal 0s and 1s so that random chance is 50%\n",
    "    n_valzero = np.shape(np.where(Yval==0)[0])[0]\n",
    "    n_valone  = np.shape(np.where(Yval==1)[0])[0]\n",
    "    i_valzero = np.where(Yval==0)[0]\n",
    "    i_valone  = np.where(Yval==1)[0]\n",
    "\n",
    "    if n_valone > n_valzero:\n",
    "        isubset_valone = np.random.choice(i_valone,size=n_valzero,replace=False)\n",
    "        i_valnew = np.sort(np.append(i_valzero,isubset_valone))\n",
    "        Y_val_1D = Yval.isel(time_all=i_valnew,drop=True)\n",
    "        X_val_w_NANS  = Xval[i_valnew]#.stack(z=('lat','lon'))\n",
    "    elif n_valone < n_valzero:\n",
    "        isubset_valzero = np.random.choice(i_valzero,size=n_valone,replace=False)\n",
    "        i_valnew = np.sort(np.append(isubset_valzero,i_valone))\n",
    "        Y_val_1D = Yval.isel(time_all=i_valnew,drop=True)\n",
    "        X_val_w_NANS  = Xval[i_valnew]#.stack(z=('lat','lon'))\n",
    "    else:\n",
    "        X_val_w_NANS = Xval#.stack(z=('lat','lon'))\n",
    "\n",
    "    #Testing\n",
    "    # make Ytest have equal 0s and 1s so that random chance is 50%\n",
    "    n_valzero = np.shape(np.where(Ytest==0)[0])[0]\n",
    "    n_valone  = np.shape(np.where(Ytest==1)[0])[0]\n",
    "    i_valzero = np.where(Ytest==0)[0]\n",
    "    i_valone  = np.where(Ytest==1)[0]\n",
    "\n",
    "    if n_valone > n_valzero:\n",
    "        isubset_valone = np.random.choice(i_valone,size=n_valzero,replace=False)\n",
    "        i_valnew = np.sort(np.append(i_valzero,isubset_valone))\n",
    "        Y_test_1D = Ytest.isel(time=i_valnew,drop=True)\n",
    "        X_test_w_NANS  = Xtest[i_valnew]#.stack(z=('lat','lon'))\n",
    "    elif n_valone < n_valzero:\n",
    "        isubset_valzero = np.random.choice(i_valzero,size=n_valone,replace=False)\n",
    "        i_valnew = np.sort(np.append(isubset_valzero,i_valone))\n",
    "        Y_test_1D = Ytest.isel(time=i_valnew,drop=True)\n",
    "        X_test_w_NANS  = Xtest[i_valnew]#.stack(z=('lat','lon'))\n",
    "    else:\n",
    "        X_test_w_NANS = Xtest#.stack(z=('lat','lon'))\n",
    "\n",
    "    X_train_stack = X_train_w_NANS.stack(z=('lat','lon'))\n",
    "    X_train = X_train_stack.dropna(dim=\"z\", how=\"any\")\n",
    "\n",
    "    X_val_stack = X_val_w_NANS.stack(z=('lat','lon'))\n",
    "    X_val = X_val_stack.dropna(dim=\"z\", how=\"any\")\n",
    "\n",
    "    # X_test_stack = X_test_w_NANS.stack(z=('lat','lon'))\n",
    "    # X_test = X_test_stack.dropna(dim=\"z\", how=\"any\")\n",
    "\n",
    "    X_test_stack = Xtest.stack(z=('lat','lon'))\n",
    "    X_test = X_test_stack.dropna(dim=\"z\", how=\"all\")\n",
    "\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    Y_train      = enc.fit_transform(np.array(Y_train_1D).reshape(-1, 1)).toarray()\n",
    "    Y_val  = enc.fit_transform(np.array(Y_val_1D).reshape(-1, 1)).toarray()\n",
    "    #Y_test  = enc.fit_transform(np.array(Y_test_1D).reshape(-1, 1)).toarray()\n",
    "    Y_test  = enc.fit_transform(np.array(Ytest).reshape(-1, 1)).toarray()\n",
    "\n",
    "    del Y_val1_inputtime, Yval, Y_val1_inputtime1, Ytrain\n",
    "    del X_val1_inputtime1, X_all1_inputtime\n",
    "    del val_centered_weighted_rolling_avg, centered_weighted_rolling_avg\n",
    "\n",
    "    # Evaluate Test Member and Metrics\n",
    "\n",
    "    perc = 80 #(100-perc)% of predictions\n",
    "    conf_level = 'most'\n",
    "    subset = 'correct'\n",
    "    X1= X_test.copy(deep=True)\n",
    "\n",
    "    #>>>>>>>>Analyze test data on trained model<<<<<<<<\n",
    "    NETWORK_SEED=0\n",
    "    model, LOSS = make_model() \n",
    "    ind = 0\n",
    "\n",
    "    stored_accuracy = np.zeros(len(RANDOM_SEED))\n",
    "    for SEED in RANDOM_SEED:\n",
    "        #if ind < num_experiments:\n",
    "        model.load_weights(ddir_MODEL+PREDICTOR_VAR+'_'+str(lead_weeks)+'wklead_operationalseed_testens'+str(testing_ens)+'_seed'+str(SEED)+'.h5')\n",
    "        results_test = model.evaluate(X_test,Y_test,verbose = 0) #prints out model evaluation \n",
    "        stored_accuracy[ind] = results_test[2]\n",
    "        ind += 1\n",
    "\n",
    "    seed_winner_trained = np.argmax(np.array(stored_accuracy[:]))\n",
    "\n",
    "    for SEED in [RANDOM_SEED[seed_winner_trained]]:\n",
    "        #**** To evaluate XAI on validation or testing, just change Ptest and Cttest_true)********\n",
    "\n",
    "        Ptest = model.predict(X_test)    #predicted values on test data - softmax output of confidence \n",
    "        Cptest_pred = Ptest.argmax(axis=1)     #0,1 of predicted  \n",
    "        Cttest_true = Y_test.argmax(axis=1) #true values on validation \n",
    "\n",
    "        #<<<<<<<<<<<Split most confident data into correct and incorrect>>>>>>>\n",
    "        #softmax:\n",
    "\n",
    "        max_logits = np.max(Ptest,axis=-1)\n",
    "        if conf_level == 'most':\n",
    "            i_cover = np.where(max_logits >= np.percentile(max_logits, perc))[0]\n",
    "\n",
    "        if conf_level == 'least':\n",
    "            i_cover = np.where(max_logits < np.percentile(max_logits, perc))[0]\n",
    "\n",
    "        cat_true_conf = Cttest_true[i_cover]\n",
    "        cat_pred_conf = Cptest_pred[i_cover]\n",
    "\n",
    "        #if subset == 'correct':\n",
    "        #location of correct predictions\n",
    "        icorr = np.where(Cptest_pred[i_cover] - Cttest_true[i_cover] == 0)[0]\n",
    "        X1_subset = X1[i_cover][icorr] #index Xtest with indicies of CONFIDENT & CORRECT predictions; for output compositing, replace X1 with Y variable \n",
    "        Y_true_subset = cat_true_conf[icorr]\n",
    "        Y_pred_subset = cat_pred_conf[icorr]\n",
    "        time = X_test.time[icorr]\n",
    "\n",
    "        #if subset == 'incorrect': #confident but incorrect/wrong prediction\n",
    "        #location of incorrect predictions\n",
    "        i_incorr = np.where(Cptest_pred[i_cover] - Cttest_true[i_cover] != 0)[0]\n",
    "        X1_subset_F = X1[i_cover][i_incorr] #index Xtest with indicies of INCORRECT predictions\n",
    "        Y_false_subset = cat_true_conf[i_incorr]\n",
    "        Y_pred_subset = cat_pred_conf[i_incorr]\n",
    "        time_F = X_test.time[i_incorr]\n",
    "\n",
    "        sample = X1_subset.copy()\n",
    "        icat = np.where(Y_true_subset == 0)[0]\n",
    "        time_class = time[icat]\n",
    "        true_neg = X1_subset[icat]\n",
    "\n",
    "        sample = X1_subset.copy()\n",
    "        icat = np.where(Y_true_subset == 1)[0]\n",
    "        time_class = time[icat]\n",
    "        true_pos = X1_subset[icat]\n",
    "\n",
    "        sample = X1_subset_F.copy()\n",
    "        icat = np.where(Y_false_subset == 1)[0]  # \n",
    "        time_class = time_F[icat]\n",
    "        false_neg = X1_subset_F[icat]\n",
    "\n",
    "        sample = X1_subset_F.copy()\n",
    "        icat = np.where(Y_false_subset == 0)[0]  # \n",
    "        time_class = time_F[icat]\n",
    "        false_pos = X1_subset_F[icat]\n",
    "        \n",
    "        chance_hit = int((len(true_pos) + len(false_pos))*(len(true_pos) + len(false_neg))/len(cat_true_conf))\n",
    "        \n",
    "        \n",
    "    threat_score_value = metrics.threat_score(len(true_pos), len(false_pos), len(false_neg))\n",
    "    print(\"Threat Score:\", threat_score_value)\n",
    "    \n",
    "    gilbert_score_value = metrics.gilbert_skill_score(len(true_pos), len(false_pos), len(false_neg), chance_hit)\n",
    "    print(\"Gilbert Skill Score:\", gilbert_score_value)\n",
    "\n",
    "    threat_score_l8[exp_ind] = threat_score_value\n",
    "    gilbert_skill_score_l8[exp_ind] = gilbert_score_value\n",
    "    exp_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02796421,  0.06510851, -0.00462963,  0.00575448,  0.04327301])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gilbert_skill_score_l8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threat_xr8 = xr.DataArray(threat_score_l8)\n",
    "#threat_xr8.to_netcdf(ddir_saveinfo+'threat_score_l8_FOO.nc', mode='w')\n",
    "\n",
    "gilbert_xr8 = xr.DataArray(gilbert_skill_score_l8)\n",
    "gilbert_xr8.to_netcdf(ddir_saveinfo+'gilbert_skill_score_l8_FOO.nc', mode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(chance_hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2460,)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_true_conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "chance_hitX = (len(true_pos) + len(false_pos))*(len(true_pos) + len(false_neg))/(len(cat_true_conf)*len(cat_true_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14628908057373258"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chance_hitX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2460"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_pos) + len(false_pos)+len(true_neg)+len(false_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 265)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1007"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(false_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(false_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
